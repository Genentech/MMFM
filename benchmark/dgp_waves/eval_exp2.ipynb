{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluations DGP Waves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from mmfm.utils_eval import (\n",
    "    load_all_fm_models,\n",
    "    process_all_fm_models,\n",
    "    get_model_battery_of_best_models,\n",
    "    predict_on_testset_mmfm,\n",
    "    predict_on_testset_fsi,\n",
    "    plot_results,\n",
    ")\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mmfm.data import dgp_waves_data\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROD = True\n",
    "minimum_seeds = 5\n",
    "dimension = 2\n",
    "batch_size = None\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "path_name_fsi = \"/home/rohbeckm/scratch/results/dgp_waves/results_fsi\"\n",
    "\n",
    "# Data\n",
    "DGP = \"e\"\n",
    "eval_class = 5.0\n",
    "label_list = np.linspace(1, 10, 10)\n",
    "\n",
    "ns_per_t_and_c = 50\n",
    "coupling = \"cot\"\n",
    "data_std = 0.025\n",
    "off_diagonal = 0.0\n",
    "train_test_split = 0.5\n",
    "embedding_type = \"free\"\n",
    "classifier_free = False\n",
    "model_type = \"mmfm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all MMFM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False & (Path(\"data\") / f\"df_gt_{DGP}.parquet\").exists():\n",
    "    df = pd.read_parquet(Path(\"data\") / f\"df_gt_{DGP}.parquet\")\n",
    "    # Try to convert each value in column flow_variance to float if possible, otherwise leave value as string\n",
    "    try:\n",
    "        df[\"flow_variance\"] = df[\"flow_variance\"].astype(float)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    with open(Path(\"data\") / f\"grouping_columns_{DGP}.pkl\", \"rb\") as f:\n",
    "        grouping_columns = pickle.load(f)\n",
    "    with open(Path(\"data\") / f\"performance_columns_{DGP}.pkl\", \"rb\") as f:\n",
    "        performance_columns = pickle.load(f)\n",
    "else:\n",
    "    df, grouping_columns, performance_columns = load_all_fm_models(\n",
    "        path=\"/home/rohbeckm/scratch/results/dgp_waves/results_mmfm\",\n",
    "        production=PROD,\n",
    "        dgp=DGP,\n",
    "        embedding_type=embedding_type,\n",
    "        coupling=coupling,\n",
    "    )\n",
    "\n",
    "    # Save everything\n",
    "    # Convert\n",
    "    df[\"flow_variance\"] = df[\"flow_variance\"].astype(str)\n",
    "    df.to_parquet(Path(\"data\") / f\"df_gt_{DGP}.parquet\")\n",
    "    # Try to convert each value in column flow_variance to float if possible, otherwise leave value as string\n",
    "    try:\n",
    "        df[\"flow_variance\"] = df[\"flow_variance\"].astype(float)\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    with open(Path(\"data\") / f\"grouping_columns_{DGP}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(grouping_columns, f)\n",
    "    with open(Path(\"data\") / f\"performance_columns_{DGP}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(performance_columns, f)\n",
    "\n",
    "verbose = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess all MMFM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[\n",
    "    (df[\"ns_per_t_and_c\"] == ns_per_t_and_c)\n",
    "    & (df[\"coupling\"] == coupling)\n",
    "    & (df[\"data_std\"] == data_std)\n",
    "    & (df[\"off_diagonal\"] == off_diagonal)\n",
    "    & (df[\"train_test_split\"] == train_test_split)\n",
    "    & (df[\"embedding_type\"] == embedding_type)\n",
    "    & (df[\"model_type\"] == model_type)\n",
    "]\n",
    "\n",
    "if not classifier_free:\n",
    "    df = df.loc[~df[\"classifier_free\"]]\n",
    "\n",
    "# Filter for validation results\n",
    "df.loc[:, \"train\"] = df[\"train\"].astype(bool)\n",
    "df = df.loc[~df[\"train\"]]\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, grouping_columns, performance_columns = process_all_fm_models(\n",
    "    df,\n",
    "    grouping_columns,\n",
    "    performance_columns,\n",
    "    plot=1,\n",
    "    verbose=1,\n",
    "    minimum_seeds=minimum_seeds,\n",
    "    data_cols=[\n",
    "        \"ns_per_t_and_c\",\n",
    "        \"coupling\",\n",
    "        \"data_std\",\n",
    "        \"dgp\",\n",
    "        \"interpolation\",\n",
    "        \"guidance\",\n",
    "        \"embedding_type\",\n",
    "        \"classifier_free\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only compare to natural cubic splines on FSI later, so let's focus on\n",
    "# these interpolations for MMFM as well\n",
    "df_cubic = df.loc[(df[\"interpolation\"] == \"cubic\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find best model on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add weight column to lay focus on conditional generalization\n",
    "df_cubic[\"weight\"] = 1\n",
    "\n",
    "# this is our extra validation timepoint\n",
    "# Note, that for a fair comparison we provide this sample as training data to FSI\n",
    "add_time_cond = (5, 0.55)\n",
    "\n",
    "# Filtering for target validation time and c\n",
    "df_cubic_valid = df_cubic.loc[\n",
    "    ((df_cubic[\"c\"] == add_time_cond[0]) & (df_cubic[\"time\"].isin([add_time_cond[1]])))\n",
    "    | ((df_cubic[\"c\"] == eval_class) & (df_cubic[\"time\"].isin([0, 0.25, 0.5])))\n",
    "    | ((df_cubic[\"c\"] != eval_class) & (df_cubic[\"time\"].isin([0, 0.25, 0.5, 0.75, 1.0])))\n",
    "]\n",
    "# We focus a bit more on the validation timepoint than on the training timepoints we saw\n",
    "# during training\n",
    "df_cubic_valid = df_cubic_valid.assign(\n",
    "    weight=np.where((df_cubic_valid[\"c\"] == add_time_cond[0]) & (df_cubic_valid[\"time\"] == add_time_cond[1]), 10, 1)\n",
    ")\n",
    "\n",
    "for select_by in [\n",
    "    \"mean_diff_l2_mean\",\n",
    "]:\n",
    "    print(f\"Selecting by {select_by}\")\n",
    "    df_top_valid, model_battery, model_states, model_guidances = get_model_battery_of_best_models(\n",
    "        df_cubic_valid,\n",
    "        DGP,\n",
    "        grouping_columns,\n",
    "        select_by=select_by,\n",
    "        device=\"cuda\",\n",
    "        n_top_models=1,\n",
    "        n_top_seeds=3,\n",
    "        model_string=\"dgp_waves\",\n",
    "        label_list=label_list,\n",
    "        average_out_seed=True,\n",
    "    )\n",
    "\n",
    "    X_test, y_test, t_test, n_classes, timepoints, all_classes = dgp_waves_data(\n",
    "        coupling, batch_size, dimension, off_diagonal, data_std, ns_per_t_and_c, dgp=DGP, return_data=\"test\"\n",
    "    )\n",
    "\n",
    "    df_cubic_results_mmfm, trajectories_test_mmfm = predict_on_testset_mmfm(\n",
    "        model_battery=model_battery,\n",
    "        model_states=model_states,\n",
    "        model_guidances=model_guidances,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        t_test=t_test,\n",
    "        device=device,\n",
    "        steps=101,\n",
    "        method=\"rk4\",\n",
    "    )\n",
    "    df_cubic_results_mmfm[\"model\"] = \"COT-MMFM\"\n",
    "\n",
    "    plot_results(X_test, y_test, t_test, trajectories_test_mmfm, ncols=5, n_classes=n_classes, plot_ode=\"u_sine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate FSI models on the same TEST data\n",
    "add_time_cond = (5, 0.55)\n",
    "\n",
    "df_cubic_results_fsi = []\n",
    "for seed in range(3):\n",
    "    filename = f\"dgp_waves_{DGP}_{seed}_{ns_per_t_and_c}_{train_test_split}_{off_diagonal}_{data_std}_{dimension}\"\n",
    "    if add_time_cond:\n",
    "        filename = filename + \"_\" + re.sub(r\"[(), ]\", \"\", str(add_time_cond))\n",
    "    results_path = Path(path_name_fsi) / filename\n",
    "    print(results_path)\n",
    "\n",
    "    df_results_fsi = predict_on_testset_fsi(\n",
    "        results_path,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        t_test,\n",
    "        seed=seed,\n",
    "        coupling=coupling,\n",
    "        n_classes=len(label_list),\n",
    "        plot_results=True,\n",
    "        ncols=5,\n",
    "    )\n",
    "    df_cubic_results_fsi.append(df_results_fsi)\n",
    "\n",
    "df_cubic_results_fsi = pd.concat(df_cubic_results_fsi).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OT-CFM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate FM models\n",
    "if (Path(\"data\") / f\"df_gt_{DGP}.parquet\").exists():\n",
    "    df = pd.read_parquet(Path(\"data\") / f\"df_gt_{DGP}.parquet\")\n",
    "    # Try to convert each value in column flow_variance to float if possible, otherwise leave value as string\n",
    "    try:\n",
    "        df[\"flow_variance\"] = df[\"flow_variance\"].astype(float)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    with open(Path(\"data\") / f\"grouping_columns_{DGP}.pkl\", \"rb\") as f:\n",
    "        grouping_columns = pickle.load(f)\n",
    "    with open(Path(\"data\") / f\"performance_columns_{DGP}.pkl\", \"rb\") as f:\n",
    "        performance_columns = pickle.load(f)\n",
    "else:\n",
    "    df, grouping_columns, performance_columns = load_all_fm_models(\n",
    "        path=\"/home/rohbeckm/scratch/results/dgp_waves/results_mmfm\", production=PROD, dgp=DGP\n",
    "    )\n",
    "\n",
    "    # Save everything\n",
    "    # Convert\n",
    "    df[\"flow_variance\"] = df[\"flow_variance\"].astype(str)\n",
    "    df.to_parquet(Path(\"data\") / f\"df_gt_{DGP}.parquet\")\n",
    "    # Try to convert each value in column flow_variance to float if possible, otherwise leave value as string\n",
    "    try:\n",
    "        df[\"flow_variance\"] = df[\"flow_variance\"].astype(float)\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    with open(Path(\"data\") / f\"grouping_columns_{DGP}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(grouping_columns, f)\n",
    "    with open(Path(\"data\") / f\"performance_columns_{DGP}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(performance_columns, f)\n",
    "\n",
    "verbose = False\n",
    "\n",
    "df = df.loc[\n",
    "    (df[\"ns_per_t_and_c\"] == ns_per_t_and_c)\n",
    "    & (df[\"coupling\"] == coupling)\n",
    "    & (df[\"data_std\"] == data_std)\n",
    "    & (df[\"off_diagonal\"] == off_diagonal)\n",
    "    & (df[\"train_test_split\"] == train_test_split)\n",
    "    & (df[\"embedding_type\"] == embedding_type)\n",
    "    & (df[\"model_type\"] == \"fm\")\n",
    "]\n",
    "\n",
    "if not classifier_free:\n",
    "    df = df.loc[~df[\"classifier_free\"]]\n",
    "\n",
    "# Filter for validation results\n",
    "df.loc[:, \"train\"] = df[\"train\"].astype(bool)\n",
    "df = df.loc[~df[\"train\"]]\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "df, grouping_columns, performance_columns = process_all_fm_models(\n",
    "    df,\n",
    "    grouping_columns,\n",
    "    performance_columns,\n",
    "    plot=False,\n",
    "    verbose=False,\n",
    "    minimum_seeds=minimum_seeds,\n",
    "    data_cols=[\n",
    "        \"ns_per_t_and_c\",\n",
    "        \"coupling\",\n",
    "        \"data_std\",\n",
    "        \"dgp\",\n",
    "        \"interpolation\",\n",
    "        \"guidance\",\n",
    "        \"embedding_type\",\n",
    "        \"classifier_free\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "# We only compare to natural cubic splines on FSI later, so let's focus on\n",
    "# these interpolations for MMFM as well\n",
    "df_cubic = df.loc[(df[\"interpolation\"] == \"cubic\")]\n",
    "\n",
    "# Add weight column to lay focus on conditional generalization\n",
    "df_cubic[\"weight\"] = 1\n",
    "\n",
    "# this is our extra validation timepoint\n",
    "# Note, that for a fair comparison we provide this sample as training data to FSI\n",
    "add_time_cond = (5, 0.55)\n",
    "\n",
    "# Filtering for target validation time and c\n",
    "df_cubic_valid = df_cubic.loc[\n",
    "    ((df_cubic[\"c\"] == add_time_cond[0]) & (df_cubic[\"time\"].isin([add_time_cond[1]])))\n",
    "    | ((df_cubic[\"c\"] == eval_class) & (df_cubic[\"time\"].isin([0, 0.25, 0.5])))\n",
    "    | ((df_cubic[\"c\"] != eval_class) & (df_cubic[\"time\"].isin([0, 0.25, 0.5, 0.75, 1.0])))\n",
    "]\n",
    "# We focus a bit more on the validation timepoint than on the training timepoints we saw\n",
    "# during training\n",
    "df_cubic_valid = df_cubic_valid.assign(\n",
    "    weight=np.where((df_cubic_valid[\"c\"] == add_time_cond[0]) & (df_cubic_valid[\"time\"] == add_time_cond[1]), 100, 1)\n",
    ")\n",
    "\n",
    "for select_by in [\n",
    "    \"mean_diff_l2_mean\",\n",
    "]:\n",
    "    # for N in range(5):\n",
    "    print(f\"Selecting by {select_by}\")\n",
    "    df_top_valid, model_battery, model_states, model_guidances = get_model_battery_of_best_models(\n",
    "        df_cubic_valid,\n",
    "        DGP,\n",
    "        grouping_columns,\n",
    "        select_by=select_by,\n",
    "        device=\"cuda\",\n",
    "        n_top_models=1,\n",
    "        n_top_seeds=5,\n",
    "        model_string=\"dgp_waves\",\n",
    "        label_list=label_list,\n",
    "        average_out_seed=True,\n",
    "    )\n",
    "\n",
    "    X_test, y_test, t_test, n_classes, timepoints, all_classes = dgp_waves_data(\n",
    "        coupling, batch_size, dimension, off_diagonal, data_std, ns_per_t_and_c, dgp=DGP, return_data=\"test\"\n",
    "    )\n",
    "\n",
    "    df_cubic_results_fm, trajectories_test = predict_on_testset_mmfm(\n",
    "        model_battery=model_battery,\n",
    "        model_states=model_states,\n",
    "        model_guidances=model_guidances,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        t_test=t_test,\n",
    "        device=device,\n",
    "        steps=101,\n",
    "        method=\"rk4\",\n",
    "    )\n",
    "    df_cubic_results_fm[\"model\"] = \"FM\"\n",
    "\n",
    "    plot_results(X_test, y_test, t_test, trajectories_test, ncols=5, n_classes=n_classes, plot_ode=\"u_sine\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear COT-MMFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate FM models\n",
    "if (Path(\"data\") / f\"df_gt_{DGP}.parquet\").exists():\n",
    "    df = pd.read_parquet(Path(\"data\") / f\"df_gt_{DGP}.parquet\")\n",
    "    # Try to convert each value in column flow_variance to float if possible, otherwise leave value as string\n",
    "    try:\n",
    "        df[\"flow_variance\"] = df[\"flow_variance\"].astype(float)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    with open(Path(\"data\") / f\"grouping_columns_{DGP}.pkl\", \"rb\") as f:\n",
    "        grouping_columns = pickle.load(f)\n",
    "    with open(Path(\"data\") / f\"performance_columns_{DGP}.pkl\", \"rb\") as f:\n",
    "        performance_columns = pickle.load(f)\n",
    "else:\n",
    "    df, grouping_columns, performance_columns = load_all_fm_models(\n",
    "        path=\"/home/rohbeckm/scratch/results/dgp_waves/results_mmfm\", production=PROD, dgp=DGP\n",
    "    )\n",
    "\n",
    "    # Save everything\n",
    "    # Convert\n",
    "    df[\"flow_variance\"] = df[\"flow_variance\"].astype(str)\n",
    "    df.to_parquet(Path(\"data\") / f\"df_gt_{DGP}.parquet\")\n",
    "    # Try to convert each value in column flow_variance to float if possible, otherwise leave value as string\n",
    "    try:\n",
    "        df[\"flow_variance\"] = df[\"flow_variance\"].astype(float)\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    with open(Path(\"data\") / f\"grouping_columns_{DGP}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(grouping_columns, f)\n",
    "    with open(Path(\"data\") / f\"performance_columns_{DGP}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(performance_columns, f)\n",
    "\n",
    "verbose = False\n",
    "\n",
    "df = df.loc[\n",
    "    (df[\"ns_per_t_and_c\"] == ns_per_t_and_c)\n",
    "    & (df[\"coupling\"] == coupling)\n",
    "    & (df[\"data_std\"] == data_std)\n",
    "    & (df[\"off_diagonal\"] == off_diagonal)\n",
    "    & (df[\"train_test_split\"] == train_test_split)\n",
    "    & (df[\"embedding_type\"] == embedding_type)\n",
    "    & (df[\"model_type\"] == \"mmfm\")\n",
    "]\n",
    "\n",
    "if not classifier_free:\n",
    "    df = df.loc[~df[\"classifier_free\"]]\n",
    "\n",
    "# Filter for validation results\n",
    "df.loc[:, \"train\"] = df[\"train\"].astype(bool)\n",
    "df = df.loc[~df[\"train\"]]\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "df, grouping_columns, performance_columns = process_all_fm_models(\n",
    "    df,\n",
    "    grouping_columns,\n",
    "    performance_columns,\n",
    "    plot=False,\n",
    "    verbose=False,\n",
    "    minimum_seeds=minimum_seeds,\n",
    "    data_cols=[\n",
    "        \"ns_per_t_and_c\",\n",
    "        \"coupling\",\n",
    "        \"data_std\",\n",
    "        \"dgp\",\n",
    "        \"interpolation\",\n",
    "        \"guidance\",\n",
    "        \"embedding_type\",\n",
    "        \"classifier_free\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "# We only compare to natural cubic splines on FSI later, so let's focus on\n",
    "# these interpolations for MMFM as well\n",
    "df_cubic = df.loc[(df[\"interpolation\"] == \"linear\")]\n",
    "\n",
    "# Add weight column to lay focus on conditional generalization\n",
    "df_cubic[\"weight\"] = 1\n",
    "\n",
    "# this is our extra validation timepoint\n",
    "# Note, that for a fair comparison we provide this sample as training data to FSI\n",
    "add_time_cond = (5, 0.55)\n",
    "\n",
    "# Filtering for target validation time and c\n",
    "df_cubic_valid = df_cubic.loc[\n",
    "    ((df_cubic[\"c\"] == add_time_cond[0]) & (df_cubic[\"time\"].isin([add_time_cond[1]])))\n",
    "    | ((df_cubic[\"c\"] == eval_class) & (df_cubic[\"time\"].isin([0, 0.25, 0.5])))\n",
    "    | ((df_cubic[\"c\"] != eval_class) & (df_cubic[\"time\"].isin([0, 0.25, 0.5, 0.75, 1.0])))\n",
    "]\n",
    "# We focus a bit more on the validation timepoint than on the training timepoints we saw\n",
    "# during training\n",
    "df_cubic_valid = df_cubic_valid.assign(\n",
    "    weight=np.where((df_cubic_valid[\"c\"] == add_time_cond[0]) & (df_cubic_valid[\"time\"] == add_time_cond[1]), 100, 1)\n",
    ")\n",
    "\n",
    "for select_by in [\n",
    "    \"mean_diff_l2_mean\",\n",
    "]:\n",
    "    print(f\"Selecting by {select_by}\")\n",
    "    df_top_valid, model_battery, model_states, model_guidances = get_model_battery_of_best_models(\n",
    "        df_cubic_valid,\n",
    "        DGP,\n",
    "        grouping_columns,\n",
    "        select_by=select_by,\n",
    "        device=\"cuda\",\n",
    "        n_top_models=1,\n",
    "        n_top_seeds=5,\n",
    "        model_string=\"dgp_waves\",\n",
    "        label_list=label_list,\n",
    "        average_out_seed=True,\n",
    "    )\n",
    "\n",
    "    X_test, y_test, t_test, n_classes, timepoints, all_classes = dgp_waves_data(\n",
    "        coupling, batch_size, dimension, off_diagonal, data_std, ns_per_t_and_c, dgp=DGP, return_data=\"test\"\n",
    "    )\n",
    "\n",
    "    df_cubic_results_pcfm, trajectories_test = predict_on_testset_mmfm(\n",
    "        model_battery=model_battery,\n",
    "        model_states=model_states,\n",
    "        model_guidances=model_guidances,\n",
    "        X_test=X_test,\n",
    "        y_test=y_test,\n",
    "        t_test=t_test,\n",
    "        device=device,\n",
    "        steps=101,\n",
    "        method=\"rk4\",\n",
    "    )\n",
    "    df_cubic_results_pcfm[\"model\"] = \"PCFM\"\n",
    "\n",
    "    plot_results(X_test, y_test, t_test, trajectories_test, ncols=5, n_classes=n_classes, plot_ode=\"u_sine\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.concat(\n",
    "    [df_cubic_results_mmfm, df_cubic_results_fsi, df_cubic_results_fm, df_cubic_results_pcfm], ignore_index=True, axis=0\n",
    ").reset_index(drop=True)\n",
    "\n",
    "df_results.loc[:, \"training\"] = False\n",
    "df_results.loc[(df_results[\"c\"] != eval_class) & (df_results[\"time\"].isin([0, 0.25, 0.5, 0.75, 1.0])), \"training\"] = (\n",
    "    True\n",
    ")\n",
    "df_results.loc[(df_results[\"c\"] == eval_class) & (df_results[\"time\"].isin([0, 0.25, 0.5])), \"training\"] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results only on holdout timepoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby marginal, c, model and compute mean and std\n",
    "# and change order of multiindex\n",
    "df_results.loc[~df_results[\"training\"]].groupby([\"model\", \"marginal\"]).agg(\n",
    "    {\n",
    "        \"mmd\": [\"mean\", \"std\", \"max\"],\n",
    "        \"mmd_median\": [\"mean\", \"std\", \"max\"],\n",
    "        \"wasserstein\": [\"mean\", \"std\", \"max\"],\n",
    "        \"mean_diff_l1\": [\"mean\", \"std\", \"max\"],\n",
    "        \"mean_diff_l2\": [\"mean\", \"std\", \"max\"],\n",
    "    }\n",
    ").round(3).sort_index(axis=1).rename(columns={\"model\": \"Model\"})[\"wasserstein\"].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results on both training and holdout timepoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r = (\n",
    "    df_results.assign(class3=df_results[\"c\"] == 5)\n",
    "    .groupby([\"model\", \"training\", \"class3\"])\n",
    "    .agg(\n",
    "        {\n",
    "            \"mmd\": [\"mean\", \"std\", \"max\"],\n",
    "            \"mmd_median\": [\"mean\", \"std\", \"max\"],\n",
    "            \"wasserstein\": [\"mean\", \"std\", \"max\"],\n",
    "            \"mean_diff_l1\": [\"mean\", \"std\", \"max\"],\n",
    "            \"mean_diff_l2\": [\"mean\", \"std\", \"max\"],\n",
    "        }\n",
    "    )\n",
    "    .round(2)\n",
    "    .rename(columns={\"model\": \"Model\"})[[\"mean_diff_l2\", \"wasserstein\"]]\n",
    "    .reset_index()\n",
    ")\n",
    "df_r = df_r.loc[~df_r[\"training\"]]\n",
    "df_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put everything into one figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.integrate import odeint\n",
    "from mmfm.data import u_sine, u\n",
    "import cloudpickle\n",
    "from mmfm.utils import COLORMAP10, ThickerLine2D\n",
    "\n",
    "plt.style.use([\"science\", \"no-latex\"])\n",
    "\n",
    "params = {\n",
    "    \"axes.labelsize\": 18,\n",
    "    \"axes.titlesize\": 22,\n",
    "    \"xtick.labelsize\": 18,\n",
    "    \"ytick.labelsize\": 18,\n",
    "    \"legend.fontsize\": 18,\n",
    "    \"legend.title_fontsize\": 18,\n",
    "    \"figure.titlesize\": 30,\n",
    "}\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20, 5), sharex=True, sharey=True)  #\n",
    "\n",
    "add_time_cond = (5, 0.55)\n",
    "train_loader, X_train, y_train, t_train, X_valid, y_valid, t_valid, n_classes, label_list = dgp_waves_data(\n",
    "    coupling,\n",
    "    batch_size,\n",
    "    dimension,\n",
    "    off_diagonal,\n",
    "    data_std,\n",
    "    ns_per_t_and_c,\n",
    "    dgp=DGP,\n",
    "    return_data=\"train-valid\",\n",
    "    add_time_cond=add_time_cond,\n",
    "    filter_beginning_end=False,\n",
    ")\n",
    "\n",
    "\n",
    "def plot_background(ax, X, y, t, arrows=False, legend=False):\n",
    "    df = pd.DataFrame(X.reshape(-1, 2)).assign(condition=y.reshape(-1, 1), time=t.reshape(-1, 1))\n",
    "    df.columns = [\"x\", \"y\", \"condition\", \"time\"]\n",
    "    df = df.loc[~df[\"condition\"].isna()]\n",
    "    sns.scatterplot(data=df, x=\"x\", y=\"y\", hue=\"condition\", ax=ax, s=10, palette=COLORMAP10, legend=legend)\n",
    "    if arrows:\n",
    "        for c in label_list:\n",
    "            t = np.linspace(0, 1, 101)\n",
    "            sol = odeint(u_sine, (0, 0), t, args=(c,))\n",
    "            ax.plot(sol[:, 0], sol[:, 1], color=\"gray\", alpha=0.25)\n",
    "            for idx in [20, 40, 60, 80]:\n",
    "                ax.arrow(\n",
    "                    sol[idx, 0],\n",
    "                    sol[idx, 1],\n",
    "                    sol[idx + 1, 0] - sol[idx, 0],\n",
    "                    sol[idx + 1, 1] - sol[idx, 1],\n",
    "                    color=\"gray\",\n",
    "                    alpha=0.25,\n",
    "                    head_width=0.05,\n",
    "                    head_length=0.05,\n",
    "                    fc=\"black\",\n",
    "                    ec=\"black\",\n",
    "                )\n",
    "    return ax\n",
    "\n",
    "\n",
    "#\n",
    "# Figure 1 is true vector field\n",
    "#\n",
    "ax[0] = plot_background(ax[0], X_train, y_train, t_train, arrows=True)\n",
    "ax[0].set_title(\"True Vector Field (Phase Diagram)\\nwith Training Data\")\n",
    "\n",
    "#\n",
    "# Figure 2 shows FSI interpolation\n",
    "#\n",
    "ax[1] = plot_background(ax[1], X_train, y_train, t_train, arrows=False)\n",
    "ax[1].set_title(\"Predicted Trajectory FSI\")\n",
    "\n",
    "# Plot interpolation between one sample per condition, use a natural cubic spline\n",
    "for seed in range(1):\n",
    "    filename = f\"dgp_waves_{DGP}_{seed}_{ns_per_t_and_c}_{train_test_split}_{off_diagonal}_{data_std}_{dimension}\"\n",
    "    if add_time_cond:\n",
    "        filename = filename + \"_\" + re.sub(r\"[(), ]\", \"\", str(add_time_cond))\n",
    "    results_path = Path(path_name_fsi) / filename\n",
    "\n",
    "    if coupling == \"ot\":\n",
    "        name = \"model_ot_fsi.pkl\"\n",
    "    elif coupling == \"cot\":\n",
    "        name = \"model_cot_fsi.pkl\"\n",
    "    elif coupling == \"None\":\n",
    "        name = \"model_fsi.pkl\"\n",
    "    else:\n",
    "        raise ValueError(\"Coupling not recognized.\")\n",
    "\n",
    "    with open(results_path / name, \"rb\") as f:\n",
    "        fsi_model = cloudpickle.load(f)\n",
    "\n",
    "    # Plot trajectory\n",
    "    for idx_c, condition in enumerate([1, 5, 10]):\n",
    "        color_conditions = list(label_list).index(condition)\n",
    "        T = 100\n",
    "        trajectory = np.nan * np.ones(shape=(T, 10, 2))\n",
    "        for idx, sample in enumerate(range(5)):\n",
    "            sample_c = np.where(y_test[:, 0] == condition)[0]\n",
    "            for tx in range(T):\n",
    "                transport_c = fsi_model.interpolate_from_x0(\n",
    "                    X=X_test[sample_c[sample], 0][None, :],\n",
    "                    y=condition,\n",
    "                    t_query=tx / T,\n",
    "                )\n",
    "                trajectory[tx, idx] = transport_c\n",
    "\n",
    "        for sample in range(5):\n",
    "            for t in range(T - 1):\n",
    "                # Plot from t to t+1\n",
    "                ax[1].plot(\n",
    "                    trajectory[t : t + 2, sample, 0],\n",
    "                    trajectory[t : t + 2, sample, 1],\n",
    "                    color=COLORMAP10[color_conditions],\n",
    "                    lw=1,\n",
    "                )\n",
    "\n",
    "#\n",
    "# Figure 3 shows MMFM Predicted Trajectory\n",
    "#\n",
    "ax[2] = plot_background(ax[2], X_train, y_train, t_train, arrows=False, legend=True)\n",
    "ax[2].set_title(\"Predicted Trajectory COT-MMFM\")\n",
    "\n",
    "# Plot interpolation between one sample per condition, use a natural cubic spline\n",
    "for seed in range(1):\n",
    "    for idx, condition in enumerate([1, 5, 10]):\n",
    "        color_conditions = list(label_list).index(condition)\n",
    "        sub_trajectories = trajectories_test_mmfm[0][seed][:, ((color_conditions) * 50) : ((color_conditions + 1) * 50)]\n",
    "        for sample in range(5):\n",
    "            # sample_c = np.where(y_test[:, 0] == condition)[0]\n",
    "            for t in range(T - 1):\n",
    "                ax[2].plot(\n",
    "                    sub_trajectories[t : t + 2, sample, 0],\n",
    "                    sub_trajectories[t : t + 2, sample, 1],\n",
    "                    lw=1,\n",
    "                    color=COLORMAP10[color_conditions],\n",
    "                )\n",
    "\n",
    "# Create a single legend for the figure\n",
    "handles, labels = fig.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "fig.legend(\n",
    "    by_label.values(),\n",
    "    by_label.keys(),\n",
    "    loc=\"center left\",\n",
    "    bbox_to_anchor=(0.86, 0.5),\n",
    "    handler_map={plt.Line2D: ThickerLine2D()},\n",
    ")\n",
    "\n",
    "for k in range(3):\n",
    "    ax[k].set_xlabel(\"x\")\n",
    "    ax[k].set_ylabel(\"y\")\n",
    "    # Set x limits\n",
    "    ax[k].set_xlim(-0.15, 2.15)\n",
    "    ax[k].set_ylim(-3, 2.5)\n",
    "\n",
    "# Remove legend from third subplot\n",
    "ax[2].get_legend().remove()\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(right=0.85)\n",
    "# Add dots at integer coordinates\n",
    "for a in ax.flatten():\n",
    "    x_dots = np.linspace(-0.1, 2.1, 23)\n",
    "    y_dots = np.linspace(-3.1, 2.2, 24)\n",
    "    for x in x_dots:\n",
    "        for y in y_dots:\n",
    "            a.scatter(x, y, color=\"gray\", s=2, alpha=0.5, marker=\"+\")\n",
    "\n",
    "# Save figure as svg and png\n",
    "plt.savefig(\"/home/rohbeckm/code/mmfm/figures/fig2_waves_extrapol.png\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmfm.mmfm_utils import plot_results_mmfm\n",
    "\n",
    "train_loader, X_train, y_train, t_train, X_valid, y_valid, t_valid, n_classes, label_list = dgp_waves_data(\n",
    "    coupling, batch_size, dimension, off_diagonal, data_std, ns_per_t_and_c, dgp=DGP, return_data=\"train-valid\"\n",
    ")\n",
    "\n",
    "idx_plot = []\n",
    "for c in np.unique(y_test[:, 0]):\n",
    "    idx = np.where(y_test[:, 0] == c)[0][:2]\n",
    "    idx_plot.append(idx)\n",
    "idx_plot = [x.item() for x in np.array(idx_plot).flatten()]\n",
    "\n",
    "plot_results_mmfm(\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    t=t_train,\n",
    "    trajectory=trajectories_test_mmfm[0][0],\n",
    "    idx_plot=idx_plot,\n",
    "    n_classes=n_classes if n_classes is not None else 9,\n",
    "    title=\"\",\n",
    "    save=True,\n",
    "    filepath=\"/home/rohbeckm/code/mmfm/figures/figure_waves_mmfm_all_extrapol.png\",\n",
    "    s=5,\n",
    "    ncols=5,\n",
    "    plot_ode=\"u_sine\",\n",
    "    paper_style=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
